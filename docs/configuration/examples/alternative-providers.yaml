# Alternative LLM Providers Configuration
#
# Using cost-effective and fast alternatives:
# - Groq (ultra-fast inference)
# - Together.ai (open models)
# - OpenRouter (unified API)
# - Local Ollama (free)

models:
  # Groq - Extremely fast inference
  groq-llama:
    base_url: https://api.groq.com/openai/v1
    api_key_env: GROQ_API_KEY
    model: llama-3.1-70b-versatile
    temperature: 0.0
    max_tokens: 4000
    timeout: 30  # Groq is fast!
  
  # Together.ai - Open models, good pricing
  together-mixtral:
    base_url: https://api.together.xyz/v1
    api_key_env: TOGETHER_API_KEY
    model: mistralai/Mixtral-8x7B-Instruct-v0.1
    temperature: 0.0
    max_tokens: 4000
  
  # OpenRouter - Access to many models
  openrouter-claude:
    base_url: https://openrouter.ai/api/v1
    api_key_env: OPENROUTER_API_KEY
    model: anthropic/claude-sonnet-4
    temperature: 0.0
    max_tokens: 4000
  
  # Local Ollama - Completely free
  ollama-local:
    base_url: http://localhost:11434/v1
    model: codellama:13b
    temperature: 0.0
    max_tokens: 2000
    timeout: 120  # Local can be slower

pipelines:
  # Security with Groq (fast!)
  - name: security
    model: groq-llama
    prompt_file: prompts/security.md
    severity: blocker
  
  # Quality with Together.ai (cost-effective)
  - name: quality
    model: together-mixtral
    prompt_file: prompts/quality.md
    severity: warning
  
  # Performance with OpenRouter's Claude
  - name: performance
    model: openrouter-claude
    prompt_file: prompts/performance.md
    severity: warning
  
  # Documentation with local Ollama (free!)
  - name: documentation
    model: ollama-local
    prompt_file: prompts/docs.md
    severity: info

triggers:
  branches:
    - main
    - develop
  
  max_changes: 1500

output:
  summary: true
  inline_comments: true
  min_severity: warning
